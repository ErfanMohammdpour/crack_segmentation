Add a transfer-learning model to this crack segmentation repo. The project already has COCO loaders, BCE+Dice loss, AMP, logging, and evaluate/infer. Implement a lightweight segformer_lite model that uses a pretrained encoder from timm, plus a simple decoder/head for binary segmentation. Keep the existing CLI/config style.

0) Pre-req

Add dependency: timm (don’t break if it’s missing—error with a clear message).

Dataset: COCO per split at <DATA_ROOT>/{train,valid,test}/_annotations.coco.json. Positive class is any category containing “crack” (case-insensitive). Masks are binary; resize masks with INTER_NEAREST.

1) New model file

Create crackseg/models/segformer_lite.py with:

Backbone (encoder): Create via timm.create_model(...) with features_only=True or a compatible API to obtain multi-scale feature maps. Support at least:

ENCODER values: "segformer_b0", "mobilenet_v3_small".

Internally map common aliases if needed (e.g., map "segformer_b0" → "mit_b0" in timm if required).

Pretrained: If PRETRAINED: 1 or --pretrained 1, load pretrained weights from timm. If offline, catch the error and proceed with random init (log a warning).

Decoder/Head: A small FPN-ish or simple upsample-and-fuse decoder that upsamples the deepest feature back to the input stride, progressively fusing higher-resolution features (1×1 reduce → upsample → conv3×3). Final 1-channel logits head (sigmoid used only in eval/infer).

No U-Net concatenation requirement across encoder→decoder is fine (this is a transfer model, not the core “from-scratch” model).

Constructor signature:

class SegFormerLite(nn.Module):
    def __init__(self, encoder_name: str = "segformer_b0",
                 pretrained: bool = True,
                 in_ch: int = 3, num_classes: int = 1,
                 base: int = 32):
        ...


Print parameter count on init.

2) Model factory wiring

Where the repo selects models, register:

"segformer_lite" -> SegFormerLite


Use config/CLI to pass ENCODER, PRETRAINED, and BASE_CHANNELS (mapped to base if useful).

3) Freeze → Unfreeze (warm-up) + dual LR

Add CLI flags (and config fallbacks):

--freeze-epochs (int, default 0) → during these epochs, freeze encoder (requires_grad=False).

--lr-head (float) and --lr-encoder (float). If not provided:

default lr_head = cfg["LR"]

default lr_encoder = cfg["LR"] * 0.1

Build param groups:

enc_params = [p for n,p in model.named_parameters() if is_encoder(n)]
head_params = [p for n,p in model.named_parameters() if not is_encoder(n)]
optimizer = AdamW([
    {"params": head_params, "lr": lr_head},
    {"params": enc_params,  "lr": lr_encoder},
], weight_decay=cfg["WEIGHT_DECAY"])


Implement is_encoder(n) robustly (e.g., check module prefixes).

Training loop:

For epoch < freeze_epochs: keep encoder frozen (and optionally set encoder BN to eval).

At epoch == freeze_epochs: unfreeze encoder (requires_grad=True) and optionally reset the optimizer (or keep, up to you). Log the transition.

4) AMP + scheduler + early stopping

Reuse the project’s AMP (cfg["AMP"]), scheduler "plateau", early stopping, grad-clip, logs, and plots exactly like other models.

Save runs/segformer_lite/best.pth by validation IoU.

5) Evaluate & infer

evaluate.py: constructing the model must accept --model segformer_lite --encoder <name> --pretrained <0/1>. Load weights and compute IoU, Dice, Precision, Recall on test, using THRESHOLD from config. Save outputs/metrics_test.json.

infer.py: same threshold; save _mask.png and _overlay.png, resizing mask back with INTER_NEAREST.

6) Config keys (with defaults)

Update crackseg/config.yaml handling (do not hard-fail if missing; provide defaults):

MODEL_NAME: "segformer_lite"
ENCODER: "segformer_b0"   # or "mobilenet_v3_small"
PRETRAINED: 1
FREEZE_EPOCHS: 5
LR: 0.001
LR_ENCODER: 0.0001        # fallback if --lr-encoder not set
LR_HEAD: 0.001            # fallback if --lr-head not set
IMG_SIZE: 512
BATCH_SIZE: 8
AMP: true
THRESHOLD: 0.40

7) README block (How to run)

Append to README:

# Install backbone zoo
pip install timm

# Train with transfer (freeze→unfreeze + dual LR)
python train.py --config crackseg/config.yaml \
  --model segformer_lite --encoder segformer_b0 --pretrained 1 \
  --freeze-epochs 5 --lr-head 1e-3 --lr-encoder 1e-4

# Evaluate on test
python evaluate.py --config crackseg/config.yaml \
  --weights runs/segformer_lite/best.pth --model segformer_lite

# Inference on a folder
python infer.py --config crackseg/config.yaml \
  --weights runs/segformer_lite/best.pth --model segformer_lite \
  --input "<DATA_ROOT>/test" --save ./outputs/infer_segformer

8) Acceptance checks

Trains end-to-end with AMP on GPU and CPU fallback.

Logs/plots saved like other models; best checkpoint saved by val IoU.

Freeze→unfreeze actually toggles requires_grad on encoder params and is logged.

Dual-LR param groups are applied (inspect optimizer param groups).

Evaluate/infer produce metrics/overlays with the shared THRESHOLD.

Please implement with minimal changes outside the new model + factory/CLI plumbing, and match the repo’s code style.